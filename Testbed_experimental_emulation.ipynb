{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Testbed experimental emulation.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robertmessan/Assistant_virtuel_vocal/blob/main/Testbed_experimental_emulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrTjSwS6MMEl"
      },
      "source": [
        "from multiprocessing import Queue, Process, Event\n",
        "# If you run your code in you local machine and your local machine runs MAC-OS\n",
        "# replace \"multiprocessing\" with \"multiprocess\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import queue\n",
        "import time\n",
        "import logging\n",
        "\n",
        "# The following is to be able to mount Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "import pickle # To load the model\n",
        "\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG0PV_TF-g4P"
      },
      "source": [
        "We connect Google Drive to load the model we trained in the other notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl85JOuh-mej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df7adc39-e481-41d2-8411-d1e9db0b72a2"
      },
      "source": [
        "mount_point = '/content/gdrive' # Always the same, don't change it\n",
        "drive.mount(mount_point, force_remount=True)\n",
        "drive_path = mount_point + '/My Drive/' # Always the same, don't change it\n",
        "\n",
        "# Replace the following folder with some folder inside your google drive\n",
        "my_path = drive_path + \\\n",
        "  'ML_high_speed/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hsExt5MEetZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "097308da-33ab-4c98-dbbb-075e5f325323"
      },
      "source": [
        "! wget https://raw.githubusercontent.com/andreaaraldo/machine-learning-for-networks/master/9x.ml_highspeed_networks/generator.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-13 09:39:53--  https://raw.githubusercontent.com/andreaaraldo/machine-learning-for-networks/master/9x.ml_highspeed_networks/generator.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 176598 (172K) [text/plain]\n",
            "Saving to: ‘generator.csv’\n",
            "\n",
            "\rgenerator.csv         0%[                    ]       0  --.-KB/s               \rgenerator.csv       100%[===================>] 172.46K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-06-13 09:39:53 (6.73 MB/s) - ‘generator.csv’ saved [176598/176598]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6H9KDcVMMEy"
      },
      "source": [
        "After importing the main libraries, we can create a shared queue which simulates the physical link between two machines.\n",
        "\n",
        "\n",
        "TXGEN ---> [ shared queue ] ----> RX ----> Processing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qcC7yA1MMEz",
        "outputId": "529aa615-355e-4667-abcd-a2b6acaca2fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "shared = Queue(maxsize=1024)   # Max size of the queue\n",
        "rate = .0010 # inter-packet time (seconds);\n",
        "duration = 5\n",
        "\n",
        "# The following line should be the same as the one in the \"training and testing\"\n",
        "# notebook\n",
        "features_to_keep = ['L1-dcache-load-misses', 'L1-dcache-loads', 'L1-dcache-stores',\n",
        "       'L1-icache-load-misses', 'LLC-load-misses', 'LLC-loads',\n",
        "       'LLC-store-misses', 'LLC-stores', 'branch-load-misses', 'branch-misses',\n",
        "       'branches', 'bus-cycles', 'cache-misses', 'cache-references',\n",
        "       'context-switches', 'cpu-clock', 'cycles', 'dTLB-load-misses',\n",
        "       'dTLB-store-misses', 'dTLB-stores', 'iTLB-load-misses', 'iTLB-loads',\n",
        "       'instructions', 'minor-faults', 'node-load-misses', 'node-loads',\n",
        "       'node-store-misses', 'node-stores', 'page-faults', 'ref-cycles',\n",
        "       'task-clock']\n",
        "len(features_to_keep)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rj0TxiHVMMFG"
      },
      "source": [
        "# TX gen function\n",
        "def txgen(id, l_queue, stop_event):\n",
        "    count= 0\n",
        "    lost = 0\n",
        "\n",
        "    # Read the tx dataset and transform to numpy\n",
        "    full_df = pd.read_csv('generator.csv')\n",
        "    myfeatures = features_to_keep.copy()\n",
        "    myfeatures.insert(0,'time')\n",
        "    myfeatures.append('label')\n",
        "    df = full_df[myfeatures]\n",
        "    data = df.to_numpy()\n",
        "\n",
        "    # Limit to iterate over the dataset\n",
        "    # Feat to get rid of the labels in the csv\n",
        "    limit = len(data[:,0])\n",
        "    feat = len(data[0,:]) - 2\n",
        "\n",
        "    while (not stop_event.is_set() ):\n",
        "#    while (total_number > 0 ):\n",
        "\n",
        "        try:\n",
        "            l_queue.put_nowait( data[count%limit,:feat] )\n",
        "            #logging.debug(\"Packet added to the queue \" +str(count))\n",
        "        except queue.Full:\n",
        "            #logging.debug(\"Packet loss!\")\n",
        "            lost += 1\n",
        "\n",
        "\n",
        "        count += 1\n",
        "        #logging.debug(\"Total packet sent \" +str(count))\n",
        "        stop_event.wait(timeout=rate)#        print (\"working on %s\" % arg)\n",
        "\n",
        "    print(\"Sent: %d Lost %d \", count, lost)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj9dWjiYMMFN"
      },
      "source": [
        "# PROCESSING FUNCTION: You don't have to modify this function\n",
        "# Hint: Import the traned model, perform the classification task, and then return\n",
        "def processing(element):\n",
        "\n",
        "    ##################################\n",
        "    # Your processing goes here\n",
        "    # after loading a model\n",
        "    # use it to process the element\n",
        "    #\n",
        "    # y_pred = model(element)\n",
        "    #\n",
        "    # You should return the value of the classification task y_pred\n",
        "    #\n",
        "    # Bonus: You can also compare with the original y from the csv\n",
        "    ##################################\n",
        "\n",
        "    sample = element.reshape(-1,1).T\n",
        "    # predict_fun is to be defined later. Its implementation will depend on the\n",
        "    # model considered\n",
        "    y_pred = predict_fun(sample)\n",
        "\n",
        "\n",
        "    return (y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCRrCn83MMFV"
      },
      "source": [
        "# RX Function\n",
        "def rx(id, l_queue, stop_event):\n",
        "    #logging.debug(\"Starting the consumer\")\n",
        "    count = 0\n",
        "\n",
        "    while (not stop_event.is_set()):\n",
        "\n",
        "        try:\n",
        "            #logging.debug(\"Reading queue\")\n",
        "            pkt = l_queue.get(timeout=1)\n",
        "            #logging.debug(\"Retrieved element\")\n",
        "\n",
        "            # Processing starting... First counter\n",
        "            #t0 = time.clock()\n",
        "            count+=1\n",
        "\n",
        "            #################################\n",
        "            # Processing function. Here you have to put your ML approach\n",
        "            # Pkt is already a numpy element, including all the features but no labels\n",
        "            # The processing task is to classify the pkt\n",
        "            processing(pkt)\n",
        "            #################################\n",
        "\n",
        "            ##logging.debug(\"Elapsed time: %.6f\", time.clock() - t0)\n",
        "            #logging.debug(\"Count: %d\", count)\n",
        "\n",
        "        except Exception as e:\n",
        "            print (e)\n",
        "            pass\n",
        "    print(\"Received: %d\", count)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW-j3gSQMMFb"
      },
      "source": [
        "def main():\n",
        "    # Hint: use a global model loaded from the file where you saved your training model\n",
        "\n",
        "\n",
        "    # Event variables for the experiment\n",
        "    producer_stop = Event()\n",
        "    consumer_stop = Event()\n",
        "\n",
        "    # Logger: set the logger to level INFO for normal usage, DEBUG for detailed info\n",
        "    ##logging.basicConfig(level=#logging.INFO)\n",
        "\n",
        "    # Here we start the Traffic generator\n",
        "    t = Process(target=txgen, args=(0, shared, producer_stop))\n",
        "    t.start()\n",
        "\n",
        "    # Here we start the receiver\n",
        "    t2 = Process(target=rx, args=(0, shared, consumer_stop))\n",
        "    t2.start()\n",
        "\n",
        "    # Experiment duration\n",
        "    time.sleep(duration)\n",
        "    producer_stop.set()\n",
        "    time.sleep(1)\n",
        "    consumer_stop.set()\n",
        "\n",
        "    print(\"Generator and receiver stopped\")\n",
        "\n",
        "    t.join()\n",
        "    t2.join()\n",
        "\n",
        "    print(\"End of simulation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_oNCHyioNxq"
      },
      "source": [
        "# Before you continue\n",
        "\n",
        "After any simulation, before you try the next model, you should interrupt the Google Colab environment and start it again. Otherwise, the performance of the simulation may be impacted by some remaining operations related to the previous simulation.\n",
        "To do so, use the upper menu and:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "runtime > interrupt execution and to the following\n",
        "```\n",
        "\n",
        "Than\n",
        "\n",
        "* Do ``Runtime > Factory Reset runtime`` and run again all the cells up to this current cell (``Runtime > Run before``).\n",
        "* **Jump directly** to the other model you want to try and run from there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxNvj64kekPh"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK1RXpUlFKjQ"
      },
      "source": [
        "Load your previously trained Logistic Regressor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MytIEB0eFMl3"
      },
      "source": [
        "# Replace with your filename\n",
        "with open(my_path+\"logistic-reg.pkl\", \"rb\") as dump_file:\n",
        "  model = pickle.load(dump_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m61nX1vZiamH"
      },
      "source": [
        "We need to define the predict function. This will be used inside the simulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPT06Hg_iZRc"
      },
      "source": [
        "def predict_fun(sample):\n",
        "  y_pred = model.predict(sample)\n",
        "  return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1gSmJy_etQQ"
      },
      "source": [
        "Let's run the simulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT5erIKVevp4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5674438c-9733-471d-be9d-dfc2a648a069"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sent: %d Lost %d  4508 0\n",
            "\n",
            "Received: %d 4508\n",
            "Generator and receiver stopped\n",
            "End of simulation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5OJswlYeyPZ"
      },
      "source": [
        "# Neural Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06tnHjt7dzxH"
      },
      "source": [
        "Let's now try with our previously trained neural network.\n",
        "We first load it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW0TQAYmdyWT"
      },
      "source": [
        "nnfile = nn_file = my_path + 'nn1.h5'\n",
        "model = load_model(nn_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfX3ob4BkIgn"
      },
      "source": [
        "We redifine the prediction function. Note that the name of the function to predict in Keras is different than the scikit-learn models, like LogisticRegression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJFEm7TFjNog"
      },
      "source": [
        "def predict_fun(sample):\n",
        "  y_pred = np.argmax (model(sample, training=False) )\n",
        "  return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZJcGRtNeTtk"
      },
      "source": [
        "We now run again the `main` function, this time model is this new one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmvClVgZeJcH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dd92734-95ed-4fbf-eca3-c66ca4ff5bc8"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sent: %d Lost %d  4187 2132\n",
            "Received: %d 1268\n",
            "Generator and receiver stopped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFIQVdIalAwF"
      },
      "source": [
        "Observe that we lost a larger fraction of packets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCuUDRSre1Xc"
      },
      "source": [
        "# Your own models\n",
        "\n",
        "Try with other models (other NN architectures, other types of classifiers, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTvm_xwEMMFo"
      },
      "source": [
        "# Replace with your filename\n",
        "with open(my_path+\"svg_model.pkl\", \"rb\") as dump_file:\n",
        "  model = pickle.load(dump_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_fun(sample):\n",
        "  y_pred = model.predict(sample)\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "-gV8I2Y0-xcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFO1K6Wx-0zQ",
        "outputId": "bf9c5f85-cdaf-478f-cda2-092b66b1f91b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sent: %d Lost %d  4506 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Received: %d 4506\n",
            "Generator and receiver stopped\n",
            "End of simulation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(my_path+\"dec_tree.pkl\", \"rb\") as dump_file:\n",
        "  model = pickle.load(dump_file)"
      ],
      "metadata": {
        "id": "z8gzzrlm-7cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_fun(sample):\n",
        "  y_pred = model.predict(sample)\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "wh-cuNe--7jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSeu4Npj-7qE",
        "outputId": "e1a52b1b-8aef-4382-d92a-2965a9ac9b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sent: %d Lost %d  4502 0\n",
            "\n",
            "Received: %d 4502\n",
            "Generator and receiver stopped\n",
            "End of simulation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(my_path+\"knn_model.pkl\", \"rb\") as dump_file:\n",
        "  model = pickle.load(dump_file)"
      ],
      "metadata": {
        "id": "5pictmMy_MOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_fun(sample):\n",
        "  y_pred = model.predict(sample)\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "hZVnslPb_MT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NPMXLAp_Mi0",
        "outputId": "0b156c8b-7c25-40cb-840b-b3dbf324c96e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sent: %d Lost %d  4196 2477\n",
            "Received: %d 1858\n",
            "Generator and receiver stopped\n"
          ]
        }
      ]
    }
  ]
}